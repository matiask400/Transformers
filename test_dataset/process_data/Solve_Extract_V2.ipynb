{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --quiet langchain langchain-google-genai pandas numpy matplotlib seaborn jupyter openai\n",
    "# !pip install --upgrade  langchain-openai openai \n",
    "# !pip install langchain_community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "import openai\n",
    "from pathlib import Path\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_models = [\"gemini-1.0-pro\", \"gemini-1.5-pro\", \"gemini-1.5-flash\"]\n",
    "llm_model = llm_models[1]\n",
    "google_api_key = \"\"\n",
    "temperature = 0\n",
    "input_filename = \"../data/leetcode_problems.csv\"\n",
    "output_directory = f\"output2/temperature-{temperature}/{llm_model}/\"\n",
    "results_directory = f\"results2/temperature-{temperature}/\"\n",
    "time_limit = 60\n",
    "\n",
    "# Ensure output and results directories exist\n",
    "Path(output_directory).mkdir(parents=True, exist_ok=True)\n",
    "Path(results_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize the chat model\n",
    "chat = ChatGoogleGenerativeAI(temperature=temperature, google_api_key=google_api_key, model=llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mkoro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Model settings\n",
    "llm_models = [\"gpt-3.5-turbo\", \"gpt-4o-mini\", \"gpt-4o\", \"gpt-4-turbo\"]\n",
    "llm_model = llm_models[3]\n",
    "temperature = 1\n",
    "time_limit = 60\n",
    "\n",
    "# File and directory paths\n",
    "input_filename = \"../data/leetcode_problems.csv\"\n",
    "output_directory = f\"output2/temperature-{temperature}/{llm_model}/\"\n",
    "results_directory = f\"results2/temperature-{temperature}/\"\n",
    "\n",
    "# Ensure output and results directories exist\n",
    "Path(output_directory).mkdir(parents=True, exist_ok=True)\n",
    "Path(results_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set up the OpenAI API key\n",
    "openai_api_key = \"\"\n",
    "\n",
    "# Initialize the chat model with LangChain\n",
    "chat = ChatOpenAI(api_key=openai_api_key, model_name=llm_model, temperature=temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "    ResponseSchema(\n",
    "        name=\"problem_solution\",\n",
    "        description=\"Code a solution in Python for the given problem. \"\n",
    "            \"The solution should handle example inputs and verify if each output matches the expected result. \"\n",
    "            \"Print for each input the value `True` if the output is correct for that input, otherwise `False`. \"\n",
    "            \"Retun the solution as a Python program. \"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"input_1\",\n",
    "        description=\"Extract the first example input from the problem description. \"\n",
    "                    \"Output it formatted as input in a Python program. \"\n",
    "                    \"Do not answer if this information is not found.\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"output_1\",\n",
    "        description=\"Extract the first example output for comparison in a Python program. \"\n",
    "                    \"Do not answer if this information is not found.\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"input_2\",\n",
    "        description=\"Extract the second example input from the problem description. \"\n",
    "                    \"Output it formatted as input in a Python program. \"\n",
    "                    \"Do not answer if this information is not found.\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"output_2\",\n",
    "        description=\"Extract the second example output for comparison in a Python program. \"\n",
    "                    \"Do not answer if this information is not found.\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"input_3\",\n",
    "        description=\"Extract the third example input from the problem description. \"\n",
    "                    \"Output it formatted as input in a Python program. \"\n",
    "                    \"Do not answer if this information is not found.\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"output_3\",\n",
    "        description=\"Extract the third example output for comparison in a Python program. \"\n",
    "                    \"Do not answer if this information is not found.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Define the prompt template\n",
    "solution_template = \"\"\"\n",
    "For the following problem description, extract the following information and format it as JSON:\n",
    "\n",
    "- \"Input 1\"\n",
    "- \"Output 1\"\n",
    "- \"Input 2\"\n",
    "- \"Output 2\"\n",
    "- \"Input 3\"\n",
    "- \"Output 3\"\n",
    "- \"Problem Solution\"\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=solution_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store problems in dictionary format\n",
    "problems = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from the input CSV file\n",
    "with open(input_filename, mode='r', encoding='utf-8') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    \n",
    "    # Skip the header if needed\n",
    "    next(csv_reader)  # Skip the first line if it's a header\n",
    "    \n",
    "    # Process each row in the CSV file\n",
    "    for fields in csv_reader:\n",
    "        # Create a dictionary for this problem\n",
    "        problem = {\n",
    "            \"ID\": fields[0],\n",
    "            \"Description\": fields[2],\n",
    "        }\n",
    "        \n",
    "        # Add the problem dictionary to the list\n",
    "        problems.append(problem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to solve the problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store data\n",
    "datos = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos: []\n"
     ]
    }
   ],
   "source": [
    "print(\"Datos: {}\".format(datos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the starting lap and the maximum number of problems to solve\n",
    "start_lap = 0 # Start from the (startlap + 1) problem\n",
    "max_problems = 25 # Maximum number of problems to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mkoro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_1.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_2.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_3.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_4.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_5.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_6.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_7.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_8.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_9.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_10.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_11.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_12.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_13.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_14.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_15.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_16.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_17.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_18.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_19.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_20.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_21.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_22.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_23.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_24.py.\n",
      "Python code saved successfully in output2/temperature-1/gpt-4-turbo/output_25.py.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Limit the number of problems to solve\n",
    "for lap in range(start_lap, min(len(problems), start_lap + max_problems)):\n",
    "    try:\n",
    "        problem_text = problems[lap][\"Description\"]\n",
    "            \n",
    "        # Format messages for chat prompt\n",
    "        messages = prompt.format_messages(text=problem_text, \n",
    "                                          format_instructions=format_instructions)\n",
    "        \n",
    "        # Chat with the system and get a response\n",
    "        response = chat(messages)\n",
    "\n",
    "        # Take the time before working on the response\n",
    "        time_start = time.time()\n",
    "\n",
    "        # Parse the response content using `output_parser`\n",
    "        output_dict = output_parser.parse(response.content)\n",
    "        \n",
    "        \n",
    "        # Extract the code snippet from the response\n",
    "        python_code = output_dict.get(\"problem_solution\")\n",
    "            \n",
    "        # Process the code to omit the first and last lines\n",
    "        if \"python\" in python_code:\n",
    "            code_lines = python_code.split('\\n')[1:-1]\n",
    "            python_code = '\\n'.join(code_lines)\n",
    "        \n",
    "        # Save the processed code to a .py file\n",
    "        output_filename = f\"{output_directory}output_{lap + 1}.py\"\n",
    "        with open(output_filename, mode='w', newline='', encoding='utf-8') as pyfile:\n",
    "            pyfile.write(python_code)\n",
    "        \n",
    "        print(f\"Python code saved successfully in {output_filename}.\")\n",
    "        \n",
    "        # Execute the script and capture the output\n",
    "        script_path = output_filename\n",
    "\n",
    "        try:\n",
    "            # Start the subprocess without the timeout\n",
    "            proceso = subprocess.Popen([\"python\", script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            \n",
    "            # Use communicate with the timeout\n",
    "            salida, _ = proceso.communicate(timeout=time_limit)  # Apply the timeout here\n",
    "            \n",
    "            resultado = salida.decode().strip()\n",
    "            \n",
    "            # Count occurrences of \"True\" and \"False\" in the output\n",
    "            ocurrencias_true = resultado.count(\"True\")\n",
    "            ocurrencias_false = resultado.count(\"False\")\n",
    "            \n",
    "            # Append data to the list\n",
    "            datos.append({\n",
    "                \"ID\": lap + 1,\n",
    "                \"code\": python_code,\n",
    "                \"result\": resultado,\n",
    "                \"true_count\": ocurrencias_true,\n",
    "                \"false_count\": ocurrencias_false\n",
    "            })\n",
    "        except subprocess.TimeoutExpired:\n",
    "            proceso.kill()\n",
    "            datos.append({\n",
    "                \"ID\": lap + 1,\n",
    "                \"code\": python_code,\n",
    "                \"result\": \"TIMEOUT\",\n",
    "                \"true_count\": \"TIMEOUT\",\n",
    "                \"false_count\": \"TIMEOUT\"\n",
    "            })\n",
    "\n",
    "        except (SyntaxError, IndentationError, TypeError, ValueError, ImportError, AttributeError, KeyError, IndexError) as e:\n",
    "            # Handle specific exceptions separately\n",
    "            datos.append({\n",
    "                \"ID\": lap + 1,\n",
    "                \"code\": python_code,  # Include the problematic code\n",
    "                \"result\": str(e),\n",
    "                \"true_count\": \"ERROR\",\n",
    "                \"false_count\": \"ERROR\"\n",
    "            })\n",
    "\n",
    "        # Take the time after working on the response\n",
    "        time_end = time.time()\n",
    "        # Calculate the time elapsed\n",
    "        time_elapsed = time_end - time_start\n",
    "    \n",
    "    except Exception as e:\n",
    "        datos.append({\n",
    "            \"ID\": lap + 1,\n",
    "            \"code\": python_code,\n",
    "            \"result\": str(e),\n",
    "            \"true_count\": \"ERROR\",\n",
    "            \"false_count\": \"ERROR\"\n",
    "        })\n",
    "        \n",
    "    # Pause execution for 15 seconds before next iteration\n",
    "    if lap < start_lap + max_problems - 1:\n",
    "        try:\n",
    "            if time_elapsed < 3:\n",
    "                time.sleep(3 - time_elapsed)\n",
    "        except NameError:\n",
    "            time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: content='```json\\n{\\n    \"problem_solution\": \"class ListNode:\\\\n    def __init__(self, val=0, next=None):\\\\n        self.val = val\\\\n        self.next = next\\\\n\\\\ndef reverseKGroup(head: ListNode, k: int) -> ListNode:\\\\n    if head is None or k == 1:\\\\n        return head\\\\n    dummy = ListNode(0)\\\\n    dummy.next = head\\\\n    cur, prev, next = head, dummy, None\\\\n    count = 0\\\\n    while cur:\\\\n        cur = cur.next\\\\n        count += 1\\\\n    while count >= k:\\\\n        cur = prev.next\\\\n        next = cur.next\\\\n        for i in range(1, k):\\\\n            cur.next = next.next\\\\n            next.next = prev.next\\\\n            prev.next = next\\\\n            next = cur.next\\\\n        prev = cur\\\\n        count -= k\\\\n    return dummy.next\\\\n\\\\n# testing expected outputs\\\\nexample_1 = [1, 2, 3, 4, 5]\\\\nk_1 = 2\\\\nexpected_1 = [2, 1, 4, 3, 5]\\\\nexample_2 = [1, 2, 3, 4, 5]\\\\nk_2 = 3\\\\nexpected_2 = [3, 2, 1, 4, 5]\\\\nexample_3 = [1, 2, 3, 4, 5]\\\\nk_3 = 1\\\\nexpected_3 = [1, 2, 3, 4, 5]\\\\n\\\\nprint(reverseKGroup(example_1, k_1) == expected_1)\\\\nprint(reverseKGroup(example_2, k_2) == expected_2)\\\\nprint(reverseKGroup(example_3, k_3) == expected_3)\",\\n    \"input_1\": \"[1, 2, 3, 4, 5], 2\",\\n    \"output_1\": \"[2, 1, 4, 3, 5]\",\\n    \"input_2\": \"[1, 2, 3, 4, 5], 3\",\\n    \"output_2\": \"[3, 2, 1, 4, 5]\",\\n    \"input_3\": \"[1, 2, 3, 4, 5], 1\",\\n    \"output_3\": \"[1, 2, 3, 4, 5]\"\\n}\\n```' response_metadata={'token_usage': {'completion_tokens': 530, 'prompt_tokens': 655, 'total_tokens': 1185}, 'model_name': 'gpt-4-turbo', 'system_fingerprint': 'fp_c77e07d4ef', 'finish_reason': 'stop', 'logprobs': None} id='run-160dd1e2-c7a2-4a34-a47a-28e3eccf82c9-0'\n"
     ]
    }
   ],
   "source": [
    "print(\"Response: {}\".format(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: [HumanMessage(content='\\nFor the following problem description, extract the following information and format it as JSON:\\n\\n- \"Input 1\"\\n- \"Output 1\"\\n- \"Input 2\"\\n- \"Output 2\"\\n- \"Input 3\"\\n- \"Output 3\"\\n- \"Problem Solution\"\\n\\nText: Given a linked list, reverse the nodes of a linked list k at a time and return its modified list.\\n\\nk is a positive integer and is less than or equal to the length of the linked list. If the number of nodes is not a multiple of k then left-out nodes, in the end, should remain as it is.\\n\\nFollow up:\\nCould you solve the problem in `O(1)` extra memory space?\\nYou may not alter the values in the list\\'s nodes, only nodes itself may be changed.\\n\\n\\nExample 1:\\nInput: head = [1,2,3,4,5], k = 2\\nOutput: [2,1,4,3,5]\\n\\nExample 2:\\nInput: head = [1,2,3,4,5], k = 3\\nOutput: [3,2,1,4,5]\\n\\nExample 3:\\nInput: head = [1,2,3,4,5], k = 1\\nOutput: [1,2,3,4,5]\\n\\nExample 4:\\nInput: head = [1], k = 1\\nOutput: [1]\\n\\nConstraints:\\nThe number of nodes in the list is in the range `sz`.\\n\\n`1 <= sz <= 5000`\\n`0 <= Node.val <= 1000`\\n`1 <= k <= sz`\\n\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"problem_solution\": string  // Code a solution in Python for the given problem. The solution should handle example inputs and verify if each output matches the expected result. Print for each input the value `True` if the output is correct for that input, otherwise `False`. Retun the solution as a Python program. \\n\\t\"input_1\": string  // Extract the first example input from the problem description. Output it formatted as input in a Python program. Do not answer if this information is not found.\\n\\t\"output_1\": string  // Extract the first example output for comparison in a Python program. Do not answer if this information is not found.\\n\\t\"input_2\": string  // Extract the second example input from the problem description. Output it formatted as input in a Python program. Do not answer if this information is not found.\\n\\t\"output_2\": string  // Extract the second example output for comparison in a Python program. Do not answer if this information is not found.\\n\\t\"input_3\": string  // Extract the third example input from the problem description. Output it formatted as input in a Python program. Do not answer if this information is not found.\\n\\t\"output_3\": string  // Extract the third example output for comparison in a Python program. Do not answer if this information is not found.\\n}\\n```\\n')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Message: {}\".format(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dict: {'problem_solution': 'class ListNode:\\n    def __init__(self, val=0, next=None):\\n        self.val = val\\n        self.next = next\\n\\ndef reverseKGroup(head: ListNode, k: int) -> ListNode:\\n    if head is None or k == 1:\\n        return head\\n    dummy = ListNode(0)\\n    dummy.next = head\\n    cur, prev, next = head, dummy, None\\n    count = 0\\n    while cur:\\n        cur = cur.next\\n        count += 1\\n    while count >= k:\\n        cur = prev.next\\n        next = cur.next\\n        for i in range(1, k):\\n            cur.next = next.next\\n            next.next = prev.next\\n            prev.next = next\\n            next = cur.next\\n        prev = cur\\n        count -= k\\n    return dummy.next\\n\\n# testing expected outputs\\nexample_1 = [1, 2, 3, 4, 5]\\nk_1 = 2\\nexpected_1 = [2, 1, 4, 3, 5]\\nexample_2 = [1, 2, 3, 4, 5]\\nk_2 = 3\\nexpected_2 = [3, 2, 1, 4, 5]\\nexample_3 = [1, 2, 3, 4, 5]\\nk_3 = 1\\nexpected_3 = [1, 2, 3, 4, 5]\\n\\nprint(reverseKGroup(example_1, k_1) == expected_1)\\nprint(reverseKGroup(example_2, k_2) == expected_2)\\nprint(reverseKGroup(example_3, k_3) == expected_3)', 'input_1': '[1, 2, 3, 4, 5], 2', 'output_1': '[2, 1, 4, 3, 5]', 'input_2': '[1, 2, 3, 4, 5], 3', 'output_2': '[3, 2, 1, 4, 5]', 'input_3': '[1, 2, 3, 4, 5], 1', 'output_3': '[1, 2, 3, 4, 5]'}\n"
     ]
    }
   ],
   "source": [
    "print(\"output_dict: {}\".format(output_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'results2/temperature-1/results_gpt-4-turbo.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Write results to a CSV file\n",
    "\n",
    "nombre_archivo = f\"{results_directory}results_{llm_model}.csv\"\n",
    "encabezados = [\"ID\", \"code\", \"result\", \"true_count\", \"false_count\"]\n",
    "\n",
    "with open(nombre_archivo, mode='a', newline='') as archivo_csv:\n",
    "    escritor_csv = csv.DictWriter(archivo_csv, fieldnames=encabezados)\n",
    "    if archivo_csv.tell() == 0:\n",
    "        escritor_csv.writeheader()\n",
    "    escritor_csv.writerows(datos)\n",
    "\n",
    "print(f\"CSV file '{nombre_archivo}' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
