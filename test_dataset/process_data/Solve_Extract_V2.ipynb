{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --quiet langchain langchain-google-genai pandas numpy matplotlib seaborn jupyter openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "import openai\n",
    "from pathlib import Path\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_models = [\"gemini-1.0-pro\", \"gemini-1.5-pro\", \"gemini-1.5-flash\"]\n",
    "llm_model = llm_models[1]\n",
    "google_api_key = \"AIzaSyD8FTlCUtYp-AOwv0uCuRidehUsiIC9cm8\"\n",
    "temperature = 0\n",
    "input_filename = \"../data/leetcode_problems.csv\"\n",
    "output_directory = f\"output2/temperature-{temperature}/{llm_model}/\"\n",
    "results_directory = f\"results2/temperature-{temperature}/\"\n",
    "time_limit = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output and results directories exist\n",
    "Path(output_directory).mkdir(parents=True, exist_ok=True)\n",
    "Path(results_directory).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat model\n",
    "chat = ChatGoogleGenerativeAI(temperature=temperature, google_api_key=google_api_key, model=llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "llm_models = [\"gpt-3.5-turbo\", \"gpt-4\"]\n",
    "llm_model = llm_models[1]\n",
    "temperature = 0\n",
    "time_limit = 60\n",
    "\n",
    "# File and directory paths\n",
    "input_filename = \"../data/leetcode_problems.csv\"\n",
    "output_directory = f\"output2/temperature-{temperature}/{llm_model}/\"\n",
    "results_directory = f\"results2/temperature-{temperature}/\"\n",
    "\n",
    "# Ensure output and results directories exist\n",
    "Path(output_directory).mkdir(parents=True, exist_ok=True)\n",
    "Path(results_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set up the OpenAI API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OpenAI API key not found in environment variables.\")\n",
    "\n",
    "# Initialize the chat model with LangChain\n",
    "chat = ChatOpenAI(api_key=openai_api_key, model_name=llm_model, temperature=temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "    ResponseSchema(\n",
    "        name=\"problem_solution\",\n",
    "        description=\"Code a solution in Python for the given problem. \"\n",
    "            \"The solution should handle example inputs and verify if each output matches the expected result. \"\n",
    "            \"Print for each input the value `True` if the output is correct for that input, otherwise `False`. \"\n",
    "            \"If the solution is unknown, return `NONE`.\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"input_1\",\n",
    "        description=\"Extract the first example input from the problem description. \"\n",
    "                    \"Output it formatted as input in a Python program. \"\n",
    "                    \"Do not answer if this information is not found.\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"output_1\",\n",
    "        description=\"Extract the first example output for comparison in a Python program. \"\n",
    "                    \"Do not answer if this information is not found.\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"input_2\",\n",
    "        description=\"Extract the second example input from the problem description. \"\n",
    "                    \"Output it formatted as input in a Python program. \"\n",
    "                    \"Do not answer if this information is not found.\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"output_2\",\n",
    "        description=\"Extract the second example output for comparison in a Python program. \"\n",
    "                    \"Do not answer if this information is not found.\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"input_3\",\n",
    "        description=\"Extract the third example input from the problem description. \"\n",
    "                    \"Output it formatted as input in a Python program. \"\n",
    "                    \"Do not answer if this information is not found.\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"output_3\",\n",
    "        description=\"Extract the third example output for comparison in a Python program. \"\n",
    "                    \"Do not answer if this information is not found.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Define the prompt template\n",
    "solution_template = \"\"\"\n",
    "For the following problem description, extract the following information and format it as JSON:\n",
    "\n",
    "- \"Input 1\"\n",
    "- \"Output 1\"\n",
    "- \"Input 2\"\n",
    "- \"Output 2\"\n",
    "- \"Input 3\"\n",
    "- \"Output 3\"\n",
    "- \"Problem Solution\"\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=solution_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store problems in dictionary format\n",
    "problems = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from the input CSV file\n",
    "with open(input_filename, mode='r', encoding='utf-8') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    \n",
    "    # Skip the header if needed\n",
    "    next(csv_reader)  # Skip the first line if it's a header\n",
    "    \n",
    "    # Process each row in the CSV file\n",
    "    for fields in csv_reader:\n",
    "        # Create a dictionary for this problem\n",
    "        problem = {\n",
    "            \"ID\": fields[0],\n",
    "            \"Description\": fields[2],\n",
    "        }\n",
    "        \n",
    "        # Add the problem dictionary to the list\n",
    "        problems.append(problem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to solve the problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store data\n",
    "datos = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the starting lap and the maximum number of problems to solve\n",
    "start_lap = 1 # Start from the (startlap + 1) problem\n",
    "max_problems = 25 # Maximum number of problems to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_2.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_3.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_4.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_5.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_6.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_7.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_8.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_9.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_10.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_11.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_12.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_13.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_14.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_15.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_16.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_17.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_18.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_19.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_20.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_21.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_22.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_23.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_24.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_25.py.\n",
      "Python code saved successfully in output2/temperature-0/gemini-1.5-pro/output_26.py.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Limit the number of problems to solve\n",
    "for lap in range(start_lap, min(len(problems), start_lap + max_problems)):\n",
    "    try:\n",
    "        problem_text = problems[lap][\"Description\"]\n",
    "            \n",
    "        # Format messages for chat prompt\n",
    "        messages = prompt.format_messages(text=problem_text, \n",
    "                                          format_instructions=format_instructions)\n",
    "        \n",
    "        # Chat with the system and get a response\n",
    "        response = chat(messages)\n",
    "\n",
    "        # Take the time before working on the response\n",
    "        time_start = time.time()\n",
    "\n",
    "        # Parse the response content using `output_parser`\n",
    "        output_dict = output_parser.parse(response.content)\n",
    "        \n",
    "        \n",
    "        # Extract the code snippet from the response\n",
    "        python_code = output_dict.get(\"problem_solution\")\n",
    "            \n",
    "        # Process the code to omit the first and last lines\n",
    "        if \"python\" in python_code:\n",
    "            code_lines = python_code.split('\\n')[1:-1]\n",
    "            python_code = '\\n'.join(code_lines)\n",
    "        \n",
    "        # Save the processed code to a .py file\n",
    "        output_filename = f\"{output_directory}output_{lap + 1}.py\"\n",
    "        with open(output_filename, mode='w', newline='', encoding='utf-8') as pyfile:\n",
    "            pyfile.write(python_code)\n",
    "        \n",
    "        print(f\"Python code saved successfully in {output_filename}.\")\n",
    "        \n",
    "        # Execute the script and capture the output\n",
    "        script_path = output_filename\n",
    "\n",
    "        try:\n",
    "            # Start the subprocess without the timeout\n",
    "            proceso = subprocess.Popen([\"python\", script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            \n",
    "            # Use communicate with the timeout\n",
    "            salida, _ = proceso.communicate(timeout=time_limit)  # Apply the timeout here\n",
    "            \n",
    "            resultado = salida.decode().strip()\n",
    "            \n",
    "            # Count occurrences of \"True\" and \"False\" in the output\n",
    "            ocurrencias_true = resultado.count(\"True\")\n",
    "            ocurrencias_false = resultado.count(\"False\")\n",
    "            \n",
    "            # Append data to the list\n",
    "            datos.append({\n",
    "                \"ID\": lap + 1,\n",
    "                \"code\": python_code,\n",
    "                \"result\": resultado,\n",
    "                \"true_count\": ocurrencias_true,\n",
    "                \"false_count\": ocurrencias_false\n",
    "            })\n",
    "        except subprocess.TimeoutExpired:\n",
    "            proceso.kill()\n",
    "            datos.append({\n",
    "                \"ID\": lap + 1,\n",
    "                \"code\": python_code,\n",
    "                \"result\": \"TIMEOUT\",\n",
    "                \"true_count\": \"TIMEOUT\",\n",
    "                \"false_count\": \"TIMEOUT\"\n",
    "            })\n",
    "\n",
    "        # Take the time after working on the response\n",
    "        time_end = time.time()\n",
    "        # Calculate the time elapsed\n",
    "        time_elapsed = time_end - time_start\n",
    "    \n",
    "    except Exception as e:\n",
    "        datos.append({\n",
    "            \"ID\": lap + 1,\n",
    "            \"code\": str(e),\n",
    "            \"result\": \"ERROR\",\n",
    "            \"true_count\": \"ERROR\",\n",
    "            \"false_count\": \"ERROR\"\n",
    "        })\n",
    "        \n",
    "    # Pause execution for 15 seconds before next iteration\n",
    "    if lap < start_lap + max_problems - 1:\n",
    "        try:\n",
    "            if time_elapsed < 10:\n",
    "                time.sleep(10 - time_elapsed)\n",
    "        except NameError:\n",
    "            time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'results2/temperature-0/results_gemini-1.5-pro.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Write results to a CSV file\n",
    "\n",
    "nombre_archivo = f\"{results_directory}results_{llm_model}.csv\"\n",
    "encabezados = [\"ID\", \"code\", \"result\", \"true_count\", \"false_count\"]\n",
    "\n",
    "with open(nombre_archivo, mode='a', newline='') as archivo_csv:\n",
    "    escritor_csv = csv.DictWriter(archivo_csv, fieldnames=encabezados)\n",
    "    if archivo_csv.tell() == 0:\n",
    "        escritor_csv.writeheader()\n",
    "    escritor_csv.writerows(datos)\n",
    "\n",
    "print(f\"CSV file '{nombre_archivo}' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
