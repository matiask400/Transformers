{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --quiet langchain-google-genai\n",
    "# !pip install --upgrade --quiet  langchain-google-genai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"gemini-1.5-flash\"\n",
    "google_api_key = \"AIzaSyBQPur-fpUJiU1ozsmTUkRkRHO33OqB9ak\"\n",
    "temperature = 1\n",
    "chat = ChatGoogleGenerativeAI(temperature=1, google_api_key=google_api_key, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_solution_schema = ResponseSchema(name=\"problem_solution\",\n",
    "                                        description=\"Code a solution in python for these problem \\\n",
    "                                        Make it to ask for inputs data examples \\\n",
    "                                        Answer with the solution, if you dont know the solution output NONE. \\\n",
    "                                        Make it to execute with the inputs examples and check \\\n",
    "                                        if returns the output, in case that yes it should say True, else False. \\\n",
    "                                        \")\n",
    "input_1_schema = ResponseSchema(name=\"input_1\",\n",
    "                                description=\"Extract the input 1 example \\\n",
    "                                and output them as they come for a input in a python program, dont \\\n",
    "                                answer If this information is not found. \\\n",
    "                                \")\n",
    "output_1_schema = ResponseSchema(name=\"output_1\",\n",
    "                                description=\"Extract the output 1 example \\\n",
    "                                and output them as they come to compere the output in a python program \\\n",
    "                                result, dont answer If this information is not found. \\\n",
    "                                \")\n",
    "input_2_schema = ResponseSchema(name=\"input_2\",\n",
    "                                description=\"Extract the input 2 example \\\n",
    "                                and output them as they come for a input in a python program, dont \\\n",
    "                                answer If this information is not found. \\\n",
    "                                \")\n",
    "output_2_schema = ResponseSchema(name=\"output_2\",\n",
    "                                description=\"Extract the output 2 example \\\n",
    "                                and output them as they come to compere the output in a python program \\\n",
    "                                result, dont answer If this information is not found. \\\n",
    "                                \")\n",
    "input_3_schema = ResponseSchema(name=\"input_3\",\n",
    "                                description=\"Extract the input 3 example \\\n",
    "                                and output them as they come for a input in a python program, dont \\\n",
    "                                answer If this information is not found. \\\n",
    "                                \")\n",
    "output_3_schema = ResponseSchema(name=\"output_3\",\n",
    "                                description=\"\"\"Extract the output 3 example \\\n",
    "                                and output them as they come to compere the output in a python program \\\n",
    "                                result, dont answer If this information is not found. \\\n",
    "                                \"\"\")\n",
    "\n",
    "response_schemas = [\n",
    "    problem_solution_schema,\n",
    "    input_1_schema,\n",
    "    output_1_schema,\n",
    "    input_2_schema,\n",
    "    output_2_schema,\n",
    "    input_3_schema,\n",
    "    output_3_schema\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "solution_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "Input 1: Extract the input 1 example \\\n",
    "and output them as they come for a input in a python program, dont answer If this information is not found.\n",
    "\n",
    "Output 1: Extract the output 1 example \\\n",
    "and output them as they come to compere the output in a python program result, dont answer If this information is not found.\n",
    "\n",
    "Input 2: Extract the input 2 example \\\n",
    "and output them as they come for a input in a python program, dont answer If this information is not found.\n",
    "\n",
    "Output 2: Extract the output 2 example \\\n",
    "and output them as they come to compere the output in a python program result, dont answer If this information is not found.\n",
    "\n",
    "Input 3: Extract the input 3 example \\\n",
    "and output them as they come for a input in a python program, dont answer If this information is not found.\n",
    "\n",
    "Output 3: Extract the output 3 example \\\n",
    "and output them as they come to compere the output in a python program result, dont answer If this information is not found.\n",
    "\n",
    "Problem solution: Code a solution in python for these problem \\\n",
    "Answer with the solution, if you dont know the solution output NONE. Make it to execute with the inputs examples and check \\\n",
    "if returns the output, in case that yes it should say True, else False.\n",
    "Format the output as JSON with the following keys:\n",
    "Problem solution\n",
    "Input 1\n",
    "Output 1\n",
    "Input 2\n",
    "Output 2\n",
    "Input 3\n",
    "Output 3\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=solution_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Input and output file names\n",
    "input_filename = \"../data/leetcode_problems.csv\"\n",
    "\n",
    "# List to store problems in dictionary format\n",
    "problems = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from the input CSV file\n",
    "with open(input_filename, mode='r', encoding='utf-8') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    \n",
    "    # Skip the header if needed\n",
    "    next(csv_reader)  # Skip the first line if it's a header\n",
    "    \n",
    "    # Process each row in the CSV file\n",
    "    for fields in csv_reader:\n",
    "        # Create a dictionary for this problem\n",
    "        problem = {\n",
    "            \"ID\": fields[0],\n",
    "            \"Description\": fields[2],\n",
    "        }\n",
    "        \n",
    "        # Add the problem dictionary to the list\n",
    "        problems.append(problem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store data\n",
    "datos = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the starting lap and the maximum number of problems to solve\n",
    "start_lap = 0\n",
    "max_problems = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mkoro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python code saved successfully in output/temperature-1/gemini-1.5-flash/output_1.py.\n",
      "Python code saved successfully in output/temperature-1/gemini-1.5-flash/output_2.py.\n",
      "Python code saved successfully in output/temperature-1/gemini-1.5-flash/output_3.py.\n",
      "Python code saved successfully in output/temperature-1/gemini-1.5-flash/output_4.py.\n",
      "Python code saved successfully in output/temperature-1/gemini-1.5-flash/output_5.py.\n",
      "Python code saved successfully in output/temperature-1/gemini-1.5-flash/output_6.py.\n",
      "Python code saved successfully in output/temperature-1/gemini-1.5-flash/output_7.py.\n",
      "Python code saved successfully in output/temperature-1/gemini-1.5-flash/output_8.py.\n",
      "Python code saved successfully in output/temperature-1/gemini-1.5-flash/output_9.py.\n",
      "Python code saved successfully in output/temperature-1/gemini-1.5-flash/output_10.py.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Limit the number of problems to solve\n",
    "for lap in range(start_lap, min(len(problems), start_lap + max_problems)):\n",
    "    try:\n",
    "        problem_text = problems[lap][\"Description\"]\n",
    "            \n",
    "        # Format messages for chat prompt\n",
    "        messages = prompt.format_messages(text=problem_text, \n",
    "                                        format_instructions=format_instructions)\n",
    "        \n",
    "        # Chat with the system and get a response\n",
    "        response = chat(messages)\n",
    "\n",
    "        # Parse the response content using `output_parser`\n",
    "        output_dict = output_parser.parse(response.content)\n",
    "        \n",
    "        \n",
    "        # Extract the code snippet from the response\n",
    "        python_code = output_dict.get(\"problem_solution\")\n",
    "            \n",
    "        # Process the code to omit the first and last lines\n",
    "        if \"python\" in python_code:\n",
    "            code_lines = python_code.split('\\n')[1:-1]\n",
    "            python_code = '\\n'.join(code_lines)\n",
    "        \n",
    "        # Save the processed code to a .py file\n",
    "        output_filename = f\"output/temperature-{temperature}/{llm_model}/output_{lap + 1}.py\"\n",
    "        with open(output_filename, mode='w', newline='', encoding='utf-8') as pyfile:\n",
    "            pyfile.write(python_code)\n",
    "        \n",
    "        print(f\"Python code saved successfully in {output_filename}.\")\n",
    "        \n",
    "        # Execute the script and capture the output\n",
    "        script_path = output_filename\n",
    "        proceso = subprocess.Popen([\"python\", script_path], stdout=subprocess.PIPE)\n",
    "        salida, _ = proceso.communicate()\n",
    "        resultado = salida.decode().strip()\n",
    "        \n",
    "        # Count occurrences of \"True\" and \"False\" in the output\n",
    "        ocurrencias_true = resultado.count(\"True\")\n",
    "        ocurrencias_false = resultado.count(\"False\")\n",
    "        \n",
    "        # Append data to the list\n",
    "        datos.append({\n",
    "            \"ID\": lap + 1,\n",
    "            \"code\": python_code,\n",
    "            \"result\": resultado,\n",
    "            \"true_count\": ocurrencias_true,\n",
    "            \"false_count\": ocurrencias_false\n",
    "        })\n",
    "    except Exception as e:\n",
    "        \n",
    "        # Append data to the list\n",
    "        datos.append({\n",
    "            \"ID\": lap + 1,\n",
    "            \"code\": e,\n",
    "            \"result\": \"ERROR\",\n",
    "            \"true_count\": \"ERROR\",\n",
    "            \"false_count\": \"ERROR\"\n",
    "        })   \n",
    "        \n",
    "    # Pause execution for 15 seconds before next iteration\n",
    "    if lap < start_lap + max_problems - 1:\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to a CSV file\n",
    "\n",
    "nombre_archivo = f\"results/temperature-{temperature}/results_{llm_model}.csv\"\n",
    "encabezados = [\"ID\", \"code\", \"result\", \"true_count\", \"false_count\"]\n",
    "\n",
    "with open(nombre_archivo, mode='a', newline='') as archivo_csv:\n",
    "    escritor_csv = csv.DictWriter(archivo_csv, fieldnames=encabezados)\n",
    "    if archivo_csv.tell() == 0:\n",
    "        escritor_csv.writeheader()\n",
    "    escritor_csv.writerows(datos)\n",
    "\n",
    "print(f\"CSV file '{nombre_archivo}' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
